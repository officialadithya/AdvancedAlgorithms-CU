\section{Lecture 1: January 14, 2025}

    \subsection{Introduction to Fine-Grained Complexity}
    
        We hope to precisely understand the complexity of several algorithmic problems beyond the coarse information given by standard complexity classes, like \(\com{P}\) and \(\com{NP}\). A central theme is proving lower bounds for the complexity of algorithms; this is a difficult task in general. We use reductions as an essential tool.
        \\
        \\
        The high-level idea behind fine-grained complexity is to explain hardness, or the lack of algorithmic progress, on fundamental computational problems by giving reductions from well-studied problems. Recall the \(\com{P}\) v. \(\com{NP}\) problem. Loosely, problems in \(\com{P}\) are ``easy,'' to solve, and problems in \(\com{NP}\) are ``easy'' to verify positive answers to. The hardest problems in \(\com{NP}\), the \(\com{NP}\)-complete problems do not admit polynomial-time algorithms unless \(\com{P}=\com{NP}\). To show that a problem is \(\com{NP}\)-complete, we show that it is in \(\com{NP}\), and exhibit a polynomial-time reduction from a known \(\com{NP}\)-complete problem. 
        \\
        \\
        The theory behind \(\com{NP}\)-completeness is robust and quite nice. But, we'd like finer results. Knowing whether a problem is in \(\com{P}\) or not doesn't really narrow down how efficiently we can solve it in practice. Even quadratic-time algorithms may be inefficient in production.
        \\
        \\
        These ideas motivate fine-grained complexity. Here, we start by taking a well-studied problem, say \(L_1\), and making a precise conjecture about the running time of optimal algorithms for \(L_1\). Then, we give an efficient reduction from \(L_1\) to a problem \(L_2\), for which we are showing hardness. We start by providing some hypotheses corresponding to certain problems, and later, we'll show fairly precise running time bounds for problems in \(\com{P}\).
        \begin{example}
            Under certain assumptions, there is no \(O(n^{2-\epsilon})\)-time algorithm for the \prob{Edit Distance} problem. This matches the \(O(n^2)\) dynamic programming algorithm.
        \end{example}
        \pagebreak
        \begin{compprob}[\(k\)-\textsc{SAT}] \label{prob:ksat}
            \vphantom
            \\
            \begin{itemize}
                \item Given a CNF formula \(\varphi\) with \(k\) literals in each clause,
                \item Decide: Does there exist a satisfying assignment?
            \end{itemize}
        \end{compprob}
        \begin{hypothesis}{\Stop\,\,Exponential Time Hypothesis (ETH)}{ETH}
            The \(3\)-\prob{SAT} problem takes \(2^{\Omega(n)}\) time.
        \end{hypothesis}
        \begin{remark*}
            Note that \(\com{P}\neq\com{NP}\) asserts that \(3\)-\prob{SAT} has no polynomial-time algorithm, whereas ETH asserts that \(3\)-\prob{SAT} has no subexponential-time algorithm. ETH implies \(\com{P}\neq\com{NP}\).
        \end{remark*}
        \begin{hypothesis}{\Stop\,\,Strong Exponential Time Hypothesis (SETH)}{SETH}
            For every \(\epsilon>0\), there exists \(k\in\mathbb{Z}^+\) such that there is no \(O\left(2^{(1-\epsilon)n}\right)=O((2-\epsilon)^n)\)-time algorithm for \(k\)-\prob{SAT}.
        \end{hypothesis}
        \begin{remark*}
            SETH, at a high level, claims that \(2^n\)-time is essentially optimal for \(k\)-\prob{SAT} for large \(k\).
        \end{remark*}
        \begin{compprob}[\(k\)-\textsc{Sum}] \label{prob:ksum}
            \vphantom
            \\
            \begin{itemize}
                \item Given arrays \(A_1,\ldots,A_k\) each with \(n\) integers in \([-n^c,n^c]\),
                \item Decide: Does there exist \(a_1\in A_1,\ldots, a_k\in A_k\) such that \(a_1+\cdots+a_k=0\)?
            \end{itemize}
            Some variants include assuming that \(A_1=\cdots=A_k\), or wanting to find \(a_1\in A_1,\ldots, a_k\in A_k\) where \(a_1+\cdots+a_{k-1}=a_k\).
        \end{compprob}
        \vphantom
        \\
        \\
        Naively, we can solve \(k\)-\textsc{Sum} by trying all possible choices \(a_i\) and checking the sum. This can be done in \(O(n^k)\) time. We can do better. For now, let \(k=3\). We start by computing all possible sums \(a_1+a_2\). Then, define \(S=\{a_1+a_2:a_1\in A_1,a_2\in A_2\}\). Sort \(A_3\). For each \(s\in S\), search in \(A_3\) for \(-s\). If \(-s\) is found, there exists a solution. Otherwise, there is no solution. This provides an \(O(n^2\log n)\) algorithm; computing the sums takes \(O(n^2)\) time, sorting takes \(O(n\log n)\) time, and the search procedure takes \(O(n^2\log n)\) time. 
        \begin{hypothesis}{\Stop\,\,No \(O(n^{2-\epsilon})\)-Time Algorithm for \(3\)-\textsc{Sum}}{nobetterthanquadratictime3sum}
            For every \(\epsilon>0\), there is no \(O\left(n^{2-\epsilon}\right)\)-time algorithm for \(3\)-\textsc{Sum}.
        \end{hypothesis}
        \begin{compprob}[\textsc{All Pairs Shortest Path (APSP)}] \label{prob:apsp}
            \vphantom
            \\
            \begin{itemize}
                \item Given a weighted graph \(G=(V,E,w)\) on \(n\) vertices,
                \item Find: The lengths of the shortest paths between every \(v_i,v_j\in V\).
            \end{itemize}
        \end{compprob}
        \vphantom
        \\
        \\
        One way to solve solve \textsc{APSP} is to run Dijkstra's algorithm on each vertex. Recall that Dijkstra's algorithm finds a single-source shortest path tree. We could also similarly use the Floyd-Warshall dynamic programming algorithm.
        \begin{hypothesis}{\Stop\,\,No \(O(n^{3-\epsilon})\)-Time Algorithm for \textsc{APSP}}{nobetterthancubictimeapsp}
            For every \(\epsilon>0\), there exists no \(O(n^{3-\epsilon})\)-time algorithm for \textsc{APSP}.
        \end{hypothesis}