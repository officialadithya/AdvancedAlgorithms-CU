\section{Lecture 10: February 13, 2025}

    \subsection{Graph Coloring}

        We introduce the notion of coloring a graph and the associated graph coloring problem. 
        \begin{definition}{\Stop\,\,\(k\)-Coloring of a Graph}{kcoloringgraph}
            Let \(G=(V,E)\) be an unweighted and undirected graph. A \(k\)-coloring of \(G\) is a function \(c:V\to\{1,\ldots,k\}\). We say \(c\) is admissible if \(c(u)\neq c(v)\) whenever \((u,v)\in E\).
        \end{definition}
        \begin{definition}{\Stop\,\,Chromatic Number of a Graph}{chromaticnumber}
            Let \(G=(V,E)\) be an unweighted and undirected graph. The chromatic number \(\chi(G)\) of \(G\) is the minimum \(k\) such that \(G\) has an admissible \(k\)-coloring.
        \end{definition}
        \begin{compprob}[\textsc{Graph \(k\)-Coloring}] \label{prob:graphcolor}
            \vphantom
            \\
            \begin{itemize}
                \item Given a unweighted and undirected graph \(G=(V,E)\) and \(k\in\mathbb{Z}^+\),
                \item Decide: Does there exist an admissible \(k\)-coloring of \(G\)?
            \end{itemize}
        \end{compprob}
        \begin{remark*}
            It is easy to see that any admissible \(k\)-coloring of \(G\) immediately partitions \(G\) into \(k\) independent sets, one for each color class.
        \end{remark*}
        \vphantom
        \\
        \\
        Note that \(k=2\) corresponds to determining whether the given graph is bipartite, easily done in polynomial time. For \(k\geq 3\), \textsc{Graph \(k\)-Coloring} is \(\com{NP}\)-complete. We follow the exposition of \cite{feige2011colorlecture} to describe fast \(k\)-coloring algorithms. Often, we'll take \(k=3\) and refer to ``colors'' \(c(V)\) as \(\{\textsc{Red}, \textsc{Green}, \textsc{Blue}\}\).
        \\
        \\
        There are \(k^n\) \(k\)-colorings of \(G\). The naive algorithm is to check all of them in \(O^*(k^n)\) time for admissiblity.
        \begin{question*}
            Can we do better?
        \end{question*}
        \begin{answer*}
            Yes, we can!
        \end{answer*}
        \vphantom
        \\
        \\
        For our first improvement, we'll observe that a tree with \(n\) vertices has \(k(k-1)^{n-1}\) admissible \(k\)-colorings. Hence, we may, in polynomial time, find a spanning tree of our graph \(G\), and then check \(k(k-1)^{n-1}\) admissible \(k\)-colorings of the spanning tree for admissiblity in \(G\). This gives rise to an \(O^*((k-1)^n)\)-time algorithm. 
        \\
        \\
        Our first nontrivial algorithm for \(3\)-coloring is given in Algorithm~\ref{alg:3color-reduce-to-2color}. We reduce \(3\)-coloring to \(2\)-coloring.
        \begin{algorithm}[H] 
            \begin{algorithmic}[1]
                \Require \(G=(V,E)\).
                \Procedure{3-Color-Reduction-2-Color}{$G$} 
                    \For{each subset \(R\subseteq V\) with \(|R|\leq\frac{n}{3}\)}
                        \State Attempt to \(2\)-color \((V\backslash R,E)\) with \textsc{Green} and \textsc{Blue}
                        \State \Return the coloring if successful.
                    \EndFor
                \EndProcedure 
            \end{algorithmic}
            \caption{3-Coloring via a Reduction to 2-Coloring}
            \label{alg:3color-reduce-to-2color}
        \end{algorithm}
        \vphantom
        \\
        \\
        We can think of \(R\subseteq V\) as a possible \textsc{Red} color class. The smallest color class must have size at most \(\frac{n}{3}\). Then, we can attempt to color the remaining edges with \textsc{Green} and \textsc{Blue}. We must check \(\sum_{i=1}^{\binom{n}{3}} \binom{n}{i}\) sets. Therefore, the running time of Algorithm~\ref{alg:3color-reduce-to-2color} is 
        \begin{align*}
            \left(\sum_{i=1}^{\binom{n}{3}} \binom{n}{i}\right)\poly(n)&\leq \binom{n}{\frac{n}{3}}\poly(n) \\
            &=\frac{n!}{\left(\frac{n}{3}\right)!\left(\frac{2n}{3}\right)!}\poly(n) \\
            &\approx 3^{\frac{n}{3}}\left(\frac{3}{2}\right)^\frac{2n}{3}\poly(n)=O^*\left(\left(\frac{27}{4}\right)^\frac{n}{3}\right)=O(1.89^n).
        \end{align*}
        Another approach is to reduce \(3\)-coloring to \(2\textsc{-SAT}\), illustrated in Algorithm~\ref{alg:3color-reduce-to-2sat}.
        \begin{algorithm}[H] 
            \begin{algorithmic}[1]
                \Require \(G=(V,E)\).
                \Procedure{3-Color-Reduction-2-SAT}{$G$} 
                    \For{\(v\in V\)}
                        \State Repeatedly randomly choose an assignment \(\ell(v)\) of two colors to \(v\)
                    \EndFor
                    \State Find a \(3\)-coloring respecting \(\ell(v_1),\ldots,\ell(v_n)\) by a reduction to \(\textsc{2-SAT}\).
                \EndProcedure 
            \end{algorithmic}
            \caption{3-Coloring via a Reduction to 2-SAT}
            \label{alg:3color-reduce-to-2sat}
        \end{algorithm}
        \vphantom
        \\
        \\
        The idea behind Algorithm~\ref{alg:3color-reduce-to-2sat} is to fix a legal \(3\)-coloring \(c^\star\) of \(G\), and for each vertex, we can randomly select two colors from \(\{\textsc{Red},\textsc{Green},\textsc{Blue}\}\). We want one of them to correspond with \(c^\star\). Then,
        \begin{equation*}
            \Pr[\text{all }n\text{ guesses agree with }c^\star]=\left(\frac{2}{3}\right)^n
        \end{equation*}
        \begin{lemma*}
            Suppose that we have guesses \(\ell(v)\in \{\textsc{Red},\textsc{Green},\textsc{Blue}\}^2\) that agree with \(c^\star\) for all \(n\) vertices. Then, we can find a \(3\)-coloring of \(G\) in \(\poly(n)\) time.
            \begin{proof}
                Construct a \textsc{2-Sat} instance with one variable \(x_i\) for each vertex \(v_i\in G\). The values that can be assigned to \(x_i\) are in one-to-one correspondence with the colors in \(\ell(v_i)\). Then, for each edge \((v_i,v_j)\), add either one or two \(2\)-clauses, depending on \(|\ell(v_i)\cap\ell(v_j)|\).
            \end{proof}
        \end{lemma*}
        \vphantom
        \\
        \\
        By our probabilistic analysis, and using a polynomial time \textsc{2-Sat} algorithm \adit{cite: e.g., papadimitriou}, we see that Algorithm~\ref{alg:3color-reduce-to-2sat} runs in \(O^*\left(\left(\frac{3}{2}\right)^n\right)\) time.
        \\
        \\
        We now provide an algorithm for \(k\)-coloring in general. But first, consider the following theorems.
        \begin{theorem}{\Stop\,\,Fast Multiplication of Multivariate Polynomials}{fastpolynomialmul}
            There exists an algorithm that multiplies polynomials \(p(x_1,\ldots,x_n)\) and \(q(x_1,\ldots,x_n)\) with degree at most \(d\) in each variable using \(O^*(2^nd)\) arithmetic operations.
            \begin{proof}
                The proof is by reduction to univariate polynomial multiplication and applying the fast Fourier transform.
            \end{proof}
        \end{theorem}
        \begin{definition}{\Stop\,\,Independent Set Polynomial of a Graph}{independentsetpolyofgraph}
            Define the independent set polynomial of \(G=([n],E)\) as
            \begin{equation*}
                p_G(x_1,\ldots,x_n)=\sum_{S\subseteq[n]}\left(\ones_{S\text{ is an independent set}}\right)\prod_{i\in S}x_i.
            \end{equation*}
        \end{definition}
        \begin{theorem}{\Stop\,\,A Necessary and Sufficient Condition for \(k\)-Colorability}{kcolorableiff}
            The graph \(G=([n],E)\) is \(k\)-colorable if and only if \(p_G(x_1,\ldots,x_n)^k\) contains the monomial \(z(x_1\cdots x_n)\) for some \(z\in\mathbb{Z}^+\).
            \begin{proof}
                A \(k\)-coloring of \(G\) is a partition of the vertices of \(G\) into \(k\) independent sets \(S_1,\ldots,S_k\). If \(S_1,\ldots,S_k\) corresponds to a legal \(k\)-coloring, then \(p_G(x_1,\ldots,x_k)\) contains each of the \(k\) monomials \(\prod_{i\in S_j}x_i\). So, \(p_G(x_1\ldots,x_n)^k\) contains \(x_1\cdots x_n\). On the other hand, if \(p_G(x_1,\ldots,x_n)^k\) contains \(x_1\cdots x_n\), then \(p_G\) contains \(k\) disjoint monomials corresponding to a partition of the input graph into \(k\) independent sets.
            \end{proof}
        \end{theorem}
        \vphantom
        \\
        \\
        The running time is \(O^*(2^n)\) to compute \(p_G\), and \(O^*(2^n)\) to compute \(p_G(x_1,\ldots,x_k)^n\). 

\pagebreak

    \section{Lecture 11: February 19, 2025}

        \subsection{Approximate Graph Coloring}

        Consider the following problem.
        \begin{compprob}[\textsc{Graph \(k\)-Coloring, Relax}] \label{prob:graphcolorrelax}
            \vphantom
            \\
            \begin{itemize}
                \item Given a unweighted and undirected graph \(G=(V,E)\) and \(k\in\mathbb{Z}^+\) such that \(G\) is \(k\)-colorable,
                \item Find: the smallest \(k'(n)\) such that finding an admissible \(k'(n)\)-coloring takes \(\poly(n)\) time.
            \end{itemize}
        \end{compprob}
        \vphantom
        \\
        \\
        Again, we focus on \(k=3\). Note trivially, that we can pick \(k'(n)=n\). We give a better algorithm, due to \cite{johnson1974worst}.
        \begin{algorithm}[H] 
            \begin{algorithmic}[1]
                \Require \(G=(V,E)\) is \(3\)-Colorable
                \Procedure{3-Color-Relax}{$G$} 
                    \State Choose \(S\subseteq V\) with \(|S|=\log n\).
                    \State Check all \(3^{|S|}\) possible colorings of \(G_{|S}\) for admissibility.
                    \State Color \(S\) according to an admissible coloring with \(3\) new colors, and recuse on \(G_{|V\backslash S}\).
                \EndProcedure 
            \end{algorithmic}
            \caption{Coloring in Polynomial Time with \(O\left(\frac{n}{\log n}\right)\) Colors, \cite{johnson1974worst}}
            \label{alg:3color-relax}
        \end{algorithm}
        \vphantom
        \\
        \\
        We recurse \(\frac{n}{\log n}\) times, and use \(3\) new colors each time. Now, we present Wigderson's algorithm, \cite{wigderson1983improving}.
        \begin{algorithm}[H] 
            \begin{algorithmic}[1]
                \Require \(G=(V,E)\) is \(3\)-Colorable.
                \Procedure{3-Color-Relax-Wigderson}{$G$} 
                    \While{\(\exists v\in V, \deg(v)\geq\sqrt{n}\) where \(v\) is not colored}
                        \State Color \(v\) with a new color. \Comment{Observe that the neighborhood \(N(v)\) of \(v\) is \(2\)-colorable.}
                        \State Color \(N(v)\) with two new colors.
                    \EndWhile
                    \State Color the remaining vertices using \(\sqrt{n}\) new colors.
                \EndProcedure 
            \end{algorithmic}
            \caption{Coloring in Polynomial Time with \(O(\sqrt{n})\) Colors, \cite{wigderson1983improving}}
            \label{alg:3color-relax-wigderson}
        \end{algorithm}
        \vphantom
        \\
        \\
        A key observation in Algorithm~\ref{alg:3color-relax-wigderson} is that we can color a graph \(G\) with maximum degree \(d\) with at most \(d+1\) colors in polynomial time. Each vertex \(v\) has at most \(d\) neighbors, so even if they are all colored with different colors, we can pick a \(d+1\)th color for \(v\). Therefore, Algorithm~\ref{alg:3color-relax-wigderson}.
        \\
        \\
        In each iteration of the while loop, we use \(3\) colors and color \(\sqrt{n}\) vertices. For the low-degree vertices, we use \(\sqrt{n}\) colors. So, in sum, we use \(3\sqrt{n}+\sqrt{n}=O(\sqrt{n})\) colors.
        \\
        \\
        We'll return to the relaxation of graph coloring presented, but first, we'll start an interlude on constraint satisfaction problems.

\pagebreak

    \subsection{Interlude on Constraint Satisfaction Problems}

        Consider the following definition.
        \begin{definition}{\Stop\,\,Constraint Satisfaction Problems (CSPs)}{csps}
            A \(k\)-ary constraint satisfaction problem (\(k\)-CSP) consists of
            \begin{enumerate}
                \item \(n\) variables \(x_1,\ldots,x_n\),
                \item a finite universe \(U\) of values that the variables can take,
                \item a family \(C\) of constraints which are functions \(f:U^k\to\{0,1\}\).
            \end{enumerate}
            An instance of a \(k\)-CSP \(C\) consists of \(m\) constraints
            \begin{equation*}
                f_1(x_{i,1},\ldots, x_{i,k}),\ldots, f_m(x_{m,1},\ldots, x_{m,k}).
            \end{equation*}
            We hope to find an assignment \(x_i=a_i\) for \(a_1,\ldots,a_n\in V\) satisfying all \(m\) constraints.
        \end{definition}
        \vphantom
        \\
        \\
        We provide some examples.
        \begin{example}[\(3\textsc{-SAT}\) as a CSP]
            We have that \(3\textsc{-SAT}\) is a \(3\)-CSP with \(U=\{0,1\}\) and constraints are disjunctions of three literals.
        \end{example}
        \begin{example}[\(3\textsc{-Coloring}\) as a CSP]
            We have that \(3\textsc{-Coloring}\) is a \(2\)-CSP with \(U=\{1,2,3\}\) and variables corresponding to colors assigned to vertices. The constraints correspond to edges and are of the form \(f(x_i,x_j)=1\) if \(x_i\neq x_j\) and \(f(x_i,x_j)=0\) if \(x_i=x_j\).
        \end{example}
        \begin{compprob}[\textsc{MaxCut}] \label{prob:maxcut}
            \vphantom
            \\
            \begin{itemize}
                \item Given a unweighted and undirected graph \(G=(V,E)\),
                \item Find: \(V^-\) and \(V^+\) with \(V^-\sqcup V^+=V\) such that \(|\{(u,v): u\in V^-,v\in V^+\}|\) is maximized.
            \end{itemize}
        \end{compprob}
        \begin{example}[\textsc{MaxCut} as a CSP]
            We have that \(\textsc{MaxCut}\) is a \(2\)-CSP with \(U=\{0,1\}\) and variables corresponding to an assignment of vertices to \(V^-\) or \(V^+\). The constraints correspond to edges and are of the form \(f(x_i,x_j)=1\) if \(x_i\neq x_j\) and \(f(x_i,x_j)=0\) if \(x_i=x_j\).
        \end{example}
        \begin{remark*}
            Note that if we wish to satisfy \(\ell\) constraints, taking \(\ell=m\) gives \(\com{NP}\)-completeness for the CSPs corresponding to \(3\textsc{-SAT}\) and \(3\textsc{-Coloring}\), but \(\ell=m\) for the \(\textsc{MaxCut}\) CSP is simply \(2\textsc{-Coloring}\), which is in \(\com{P}\).
        \end{remark*}
        \begin{remark*}
            The optimization form of a CSP hopes to satisfy as many constraints as is possible. Suppose \(\OPT\leq m\) is the maximal number of satisfied constraints, then, we call an \(\alpha\)-approximate solution satisfying at least \(\alpha\OPT\) clauses.
        \end{remark*}
        \pagebreak
        \begin{theorem}{\Stop\,\,Randomized Approximation Algorithms for CSPs}{randomizedapproxcsps}
            A random assignment of variables satisfies \(p_0m\) constraints in a CSP \(C\) in expectation where
            \begin{equation*}
                p_0=\min_{f\in C}\Pr_{(a_1,\ldots,a_k)\sim U^k}[f(a_1,\ldots,a_k)=1].
            \end{equation*}
            \begin{proof}
                Define an indicator random variable \(\ones_i\) to be \(1\) if \(f_i\) is satisfied and \(0\) otherwise. Then,
                \begin{align*}
                    \mathbb{E}[|f:f(x)=1|]&=\mathbb{E}\sum_{i=1}^m[\ones_i] \\
                    &=\sum_{i=1}^m\mathbb{E}[\ones_i] \\
                    &\geq \sum_{i=1}^m p_0 \\
                    &=p_0m,
                \end{align*}
                as desired.
            \end{proof}
        \end{theorem}
        \begin{remark*}
            For \(3\textsc{-SAT}\), random assignment gives a \(\frac{7}{8}\)-approximation algorithm, and it is known that \(\left(\frac{7}{8}+\epsilon\right)\) is \(\com{NP}\)-complete. For \textsc{MaxCut}, random assignment gives a \(\frac{1}{2}\) approximation algorithm, 
        \end{remark*}
        \begin{question*}
            Can we do better?
        \end{question*}
        \begin{answer*}
            In some cases, yes, we can!
        \end{answer*}

\pagebreak

\section{Lecture 12: February 25, 2025}

    \subsection{Semidefinite Programming and Geomans-Williamson}

        In this section, we present the results of \cite{goemans1995improvedmaxcut} for \textsc{MaxCut} and discuss relations to graph coloring. We follow the exposition of \cite{williamson2011design}. We use the technique of semidefinite programming. Recall the following definition from linear algebra.
        \begin{definition}{\Stop\,\,Positive Semidefinite Matrices}{possemidef}
            Let \(X\in\mathbb{R}^{n\times n}\). We say \(X\) is positive semidefinite if all eigenvalues of \(X\) are nonnegative. If \(X\) is positive semidefinite, we write \(X\succeq\zeros\). The set of symmetric matrices is denoted \(\mathbf{S}^n\subseteq \mathbb{R}^{n\times n}\).
        \end{definition}
        \begin{remark*}
            Equivalently, \(X\in\mathbb{R}^{n\times n}\) is positive semidefinite if and only if 
            \begin{enumerate}
                \item for all \(\vec{v}\in\mathbb{R}^n\), \(\vec{y}^\top X\vec{y}\geq0\), or 
                \item \(X=V^\top V\) for some \(V\in\mathbb{R}^{n\times n}\).
            \end{enumerate}
        \end{remark*}
        \begin{definition}{\Stop\,\,Semidefinite Programs}{sdps}
            A semidefinite program (SDP) is an optimization problem that can be written as
            \begin{align*}
                \max_{X\in \mathbf{S}^n} &\iprod{C}{X} \quad \\
                \textsf{s.t. }&AX=b, \\
                &X \succeq\zeros.
            \end{align*}
            where \(C\in \mathbf{S}^n\) and the inner product is \(\iprod{C}{X}=\trace(C^\top X)\).
        \end{definition}
        \begin{remark*}
            Semidefinite programs are just like linear programs, though in addition to having a linear objective and affine constraints, we also require that the matrix \(X\) of variables \(x_{i,j}\) is positive semidefinite.
        \end{remark*}
        \begin{theorem}{\Stop\,\,Solving SDPs}{solvingsdps}
            Given some technical conditions, we can solve SDPs to within additive error \(\epsilon>0\) in \(\poly\left(\log\left(\frac{1}{\epsilon}\right),n\right)\) time.
        \end{theorem}
        \begin{remark*}
            We'll just say that SDPs can be solved exactly in polynomial time, since this is the ``moral'' claim of Theorem~\ref{thm:solvingsdps}.
        \end{remark*}
        \vphantom
        \\
        \\
        Throughout, we will work with vector programs, those of the form
        \begin{align*}
            \max&\sum_{i,j}c_{i,j}\iprod{\vec{v}_i}{\vec{v}_j} \\
            \textsf{s.t. }&\sum_{i,j}a_{i,j,k}\iprod{\vec{v}_i}{\vec{v}_j}=b_k,\quad \forall k \\
            &\vec{v}_i\in\mathbb{R}^n.
        \end{align*}
        The above vector program is equivalent to the SDP
        \begin{align*}
            \max&\sum_{i,j}c_{i,j}x_{i,j} \\
            \textsf{s.t. }&\sum_{i,j}a_{i,j,k}x_{i,j}=b_k,\quad \forall k \\
            &x_{i,j}=x_{j,i},\quad \forall i,j \\
            &X=(x_{i,j})\succeq\zeros.
        \end{align*}
        This can be seen by establishing a correspondence between \(x_{i,j}\) in the SDP to the inner product \(\iprod{\vec{v}_i}{\vec{v}_j}\) in the vector program.
        \begin{itemize}
            \item Given a solution \(X\) to the SDP, we can turn it into a solution \(V\) to the vector program by computing \(V=[\vec{v}_1,\ldots,\vec{v}_n]\) such that \(X=V^\top V\).
            \item Given a solution \(V\) to the vector program, we can turn it into a solution \(X=V^\top V\) to the SDP.
        \end{itemize}
        Before we move to \textsc{MaxCut}, we define quadratic programs.
        \begin{definition}{\Stop\,\,Quadratic Programs}{qps}
            A quadratic program (QP) is an optimization problem that can be written as
            \begin{align*}
                \max_{\vec{x}\in\mathbb{R}^n} &\frac{1}{2}\vec{x}^\top Q\vec{x}+\vec{c}^\top\vec{x} \quad \\
                \textsf{s.t. }&A\vec{x}\leq\vec{b}.
            \end{align*}
        \end{definition}
        \vphantom
        \\
        \\
        We can formulate \textsc{MaxCut} as a quadratic program. Given a graph \(G=(V,E)\), we have the problem
        \begin{align*}
            \max\frac{1}{2}&\sum_{(i,j)\in E}(1-y_iy_j) \\
            \textsf{s.t. }&y_i\in\{-1,1\}, \quad i\in\{1,\ldots,n\}.
        \end{align*}
        Consider the following vector program relaxation to the \textsc{MaxCut} QP. We have
        \begin{align*}
            \max\frac{1}{2}&\sum_{(i,j)\in E}(1-\iprod{\vec{v}_i}{\vec{v}_j}) \\
            \textsf{s.t. }&\iprod{\vec{v}_i}{\vec{v}_j}=1, \quad i\in\{1,\ldots,n\}, \\
            &\vec{v}_i\in\mathbb{R}^n, \quad i\in\{1,\ldots,n\}.
        \end{align*}
        Any feasible solution \(\vec{y}\) to the QP can be turned into a feasible solution \(\vec{v}_1,\ldots,\vec{v}_n\in\mathbb{R}^n\) to the vector program with the same objective value; consider \(y_i\mapsto \vec{v}_i=[y_i,0,\ldots,0]^\top\). If \(\OPT\) is optimal for the QP, and hence \textsc{MaxCut}, and \(Z_{\mathsf{VP}}\) is optimal for the vector problem, then \(\OPT\leq Z_{\mathsf{VP}}\).
        \begin{remark*}
            We can think of the vector program as the \(n\)-dimensional relaxation of the \(1\)-dimensional QP. 
        \end{remark*}
        \vphantom
        \\
        \\
        We wish to obtain a \(\{-1,1\}\) assignment to vertices of \(G\). We use a ``randomized rounding'' technique. We give the Geomans-Williamson algorithm in Algorithm~\ref{alg:geomans-williamson}.
        \begin{algorithm}[H] 
            \begin{algorithmic}[1]
                \Procedure{GoemansWilliamson}{$G$} 
                    \State \(V^-\gets \emptyset\)
                    \State \(V^+\gets \emptyset\)
                    \State Solve the vector program relaxation of the \textsc{MaxCut} QP for \(V=[\vec{v}_1,\ldots,\vec{v}_n]\).
                    \State Sample \(\vec{r}\sim\{\vec{x}\in\mathbb{R}^n:||\vec{x}||_2=1\}\) uniformly at random.
                    \For{\(i\in\{1,\ldots,n\}\)}
                        \If{\(\iprod{\vec{v}_i}{\vec{r}}\leq0\)}
                            \State \(V^-\gets V^-\cup\{i\}\)
                        \Else 
                            \State \(V^+\gets V^+\cup\{i\}\)
                        \EndIf
                    \EndFor
                    \State \Return \((V^-,V^+)\)
                \EndProcedure 
            \end{algorithmic}
            \caption{Goemans-Williamson Approximation Algorithm for \textsc{MaxCut}}
            \label{alg:geomans-williamson}
        \end{algorithm}
        \vphantom
        \\
        \\
        The inner product \(\iprod{\vec{v}_i}{\vec{v}_j}\) for \(i\neq j\) corresponds to the angle formed by \(\vec{v}_i\) and \(\vec{v}_j\), and therefore to the chance that the corresponding edge \((i,j)\) is cut. Then,
        \begin{align*}
            \Pr[(i,j)\text{ is a cut edge}]&=\Pr[\proj_{\Span\{\vec{v}_i,\vec{v}_j\}}\vec{r}\text{ is }] \\
            &=\frac{2\theta}{2\pi}=\frac{\theta}{\pi}=\frac{\arccos(\iprod{\vec{v}_i}{\vec{v}_j})}{\pi}.
        \end{align*}
        Consider the following theorems.
        \begin{theorem}{\Stop\,\,The Geomans-Williamson Algorithm is an Approximation to \textsc{MaxCut}}{goemansapproxalg}
            The Geomans-Williamson Algorithm~\ref{alg:geomans-williamson} is an \(\alpha\)-approximation algorithm for \textsc{MaxCut} where
            \begin{equation*}
                \alpha=\inf_{x\in[-1,1]}\left(\frac{\arccos x}{\pi}\frac{2}{1-x}\right)\geq0.87857.
            \end{equation*}
            \begin{proof}
                Let
                \begin{equation*}
                    X_{i,j}=\begin{cases}
                        1 & (i,j)\text{ is a cut edge} \\
                        0 & \text{otherwise}
                    \end{cases}.
                \end{equation*}
                Then,
                \begin{align*}
                    \mathbb{E}[\text{number of cut edges}]=\mathbb{E}\left[\sum_{(i,j)\in E} X_{i,j}\right]&=\sum_{(i,j)\in E}\mathbb{E}[X_{i,j}] \\
                    &=\sum_{(i,j)\in E} \frac{\arccos(\iprod{\vec{v}_i}{\vec{v}_j})}{\pi} \\
                    &\geq \sum_{(i,j)\in E} \frac{\alpha}{2}(1-\iprod{\vec{v}_i}{\vec{v}_j}) \\
                    &=\alpha\sum_{(i,j)\in E} \frac{1}{2}(1-\iprod{\vec{v}_i}{\vec{v}_j}) \\
                    &=\alpha Z_{\mathsf{VP}}\geq \alpha\OPT,
                \end{align*}
                where \(Z_{\mathsf{VP}}\) and \(\OPT\) are the optimal vector program and quadratic program objectives for \textsc{MaxCut}, as desired. The calculation of \(\alpha\) is straightforward.
            \end{proof}
        \end{theorem}
        \begin{theorem}{\Stop\,\,Optimality of the Geomans-Williamson Algorithm}{optimalityofgeomanswilliamson}
            There is no \(\alpha\)-approximation algorithm for \textsc{MaxCut}
            \begin{enumerate}
                \item for \(\alpha>\frac{16}{17}\) assuming that \(\com{P}\neq\com{NP}\),
                \item for \(\alpha>\inf_{x\in[-1,1]}\left(\frac{\arccos x}{\pi}\frac{2}{1-x}\right)\) assuming that \(\com{P}\neq\com{NP}\) and the unique games conjecture, \cite{khot2002uniquegames}, is true.
            \end{enumerate}
        \end{theorem}

\pagebreak

\section{Lecture 13: February 27, 2025}

    \subsection{Semidefinite Programming and Graph Coloring}

        In this section, we use SDPs for graph coloring. Suppose we have a \(3\)-colorable graph \(G=(V,E)\). Consider the vector program
        \begin{align*}
            \min\,&\lambda \tag{\(\star\)}\\
            \textsf{s.t. }&\iprod{\vec{v}_i}{\vec{v}_j}\leq\lambda\quad \forall (i,j)\in E, \\
            &\iprod{\vec{v}_i}{\vec{v}_i}=1 \quad \forall i\in\{1,\ldots,n\}, \\
            &\vec{v}_i\in\mathbb{R}^n \quad \forall i\in\{1,\ldots,n\}.
        \end{align*}
        \begin{lemma*}
            If \(G\) is \(3\)-colorable, then the vector program in \((\star)\) always has a solution with \(\lambda\leq-\frac{1}{2}\). 
            \begin{proof}
                Observe that \(\iprod{\vec{v}_i}{\vec{v}_j}\leq-\frac{1}{2}\) if and only if \(\theta\geq\frac{2\pi}{3}\). Since \(G\) is \(3\)-colorable, we can set all \(\vec{v}_i\) to one of three vectors, say \(\vec{v}_\textsc{R}\), \(\vec{v}_\textsc{G}\), and \(\vec{v}_\textsc{B}\) such that no two adjacent \(\vec{v}_i\) and \(\vec{v}_j\) are assigned the same vector. Hence, for \((i,j)\in E\), \(\vec{v}_i\) and \(\vec{v}_j\) are given different colors. So,
                \begin{equation*}
                    \iprod{\vec{v}_i}{\vec{v}_j}=\cos\left(\frac{2\pi}{3}\right)=-\frac{1}{2}.
                \end{equation*}
                We've exhibited a feasible solution of \(-\frac{1}{2}\), so \(\lambda\leq -\frac{1}{2}\), as desired.
            \end{proof}
        \end{lemma*}
        \vphantom
        \\
        \\
        Consider the following definition.
        \begin{definition}{\Stop\,\,\(k\)-Semicoloring of a Graph}{graphsemicoloring}
            A \(k\)-semicoloring of an unweighted and undirected graph \(G=(V,E)\) is a \(k\)-coloring such that at most \(\frac{n}{4}\) edges have endpoints with the same color.
        \end{definition}
        \begin{remark*}
            The existence of a semicoloring of \(G\) implies the existence of a subgraph induced by at least \(\frac{n}{2}\) vertices that is admissibly colored.
        \end{remark*}
        \begin{lemma*}
            Given a polynomial time algorithm \(\mathcal{A}\) for semicoloring a \(3\)-colorable graph with \(O(k)\) colors, there exists a polynomial-time algorithm \(\mathcal{A}'\) to admissibly color \(G\) with \(O(k\log n)\) colors.
            \begin{proof}
                The algorithm \(\mathcal{A}'\) is given as follows. We run \(\mathcal{A}\) on \(G\) and recurse on the subgraph induced by at most \(\frac{n}{2}\) vertices that are adjacent to a vertex with the same color. Coloring the subgraph requires at most \(k\) new colors.
            \end{proof}
        \end{lemma*}
        \vphantom
        \\
        \\
        Using the randomized rounding technique, similar to Geomans-Williamson, we provide an algorithm for semicoloring. Let \(\Delta=\max_{v\in G}\deg v\).
        \begin{algorithm}[H] 
            \begin{algorithmic}[1]
                \Procedure{GraphSemicoloring}{$G$} 
                    \State Solve, for \(\vec{v}_1,\ldots,\vec{v}_n\),
                    \begin{align*}
                        \min\,&\lambda \tag{\(\star\)}\\
                        \textsf{s.t. }&\iprod{\vec{v}_i}{\vec{v}_j}\leq\lambda\quad \forall (i,j)\in E, \\
                        &\iprod{\vec{v}_i}{\vec{v}_i}=1 \quad \forall i\in\{1,\ldots,n\}, \\
                        &\vec{v}_i\in\mathbb{R}^n \quad \forall i\in\{1,\ldots,n\}.
                    \end{align*}
                    \State \(t\gets \left\lceil \log_3\Delta+2\right\rceil\)
                    \State Sample \(\vec{r}_1,\ldots,\vec{r}_t\sim\{\vec{x}\in\mathbb{R}^n:||\vec{x}||_2=1\}\)
                    \For{\(i\in\{1,\ldots,n\}\)}
                        \State Color \(\vec{v}_i\) with \([\sgn(\iprod{\vec{v}_i}{\vec{r}_1}),\ldots,\sgn(\iprod{\vec{v}_i}{\vec{r}_t})]\in\{-1,1\}^t\) \Comment{Uses at most \(2^t\) colors.}
                    \EndFor
                \EndProcedure 
            \end{algorithmic}
            \caption{Graph Semicoloring}
            \label{alg:graph-semicoloring}
        \end{algorithm}
        \begin{lemma*}
            Algorithm~\ref{alg:graph-semicoloring} gives a semicoloring of \(G\) using at most \(8\Delta^{\log_32}\) colors with probability at least \(\frac{1}{2}\).
            \begin{proof}
                We used at most \(2^t\leq 2^{\log_3\Delta+3}=8\Delta^{\log_32}\) colors. Then,
                \begin{align*}
                    \Pr[\vec{v}_i\text{ and }\vec{v}_j\text{ have the same color}]&=\prod_{k=1}^t \left(1-\frac{1}{\pi}\arccos(\iprod{\vec{v}_i}{\vec{v}_j})\right). \\
                    &=\left(1-\frac{1}{\pi}\arccos(\iprod{\vec{v}_i}{\vec{v}_j})\right)^t \\
                    &\leq \left(1-\frac{1}{\pi}\arccos\left(-\frac{1}{2}\right)\right)^t \\
                    &=\left(1-\frac{2}{3}\right)^t \\
                    &=\frac{1}{3^{\log_3\Delta+2}}=\frac{1}{9\Delta},
                \end{align*}
                as desired.
            \end{proof}
        \end{lemma*}